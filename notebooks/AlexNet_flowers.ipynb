{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexnet implementation in Tensorflow\n",
    "source: https://github.com/allenai/XNOR-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, paths, labels, train, test, valid, batch, seed=777):\n",
    "        l = Dataset.one_hot(labels)\n",
    "        self.train = Dataset.preprocess([paths[i] for i in train]), np.array([l[i] for i in train])\n",
    "        self.test = Dataset.preprocess([paths[i] for i in test], True), np.array([l[i] for i in test])\n",
    "        self.valid = Dataset.preprocess([paths[i] for i in valid], True), np.array([l[i] for i in valid])\n",
    "        self.train_next = 0\n",
    "        self.batch = batch\n",
    "        self.starting = self.train\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    def one_hot(labels):\n",
    "        size = len(labels)\n",
    "        max_value = max(labels)\n",
    "        one_hot = np.zeros((size, max_value+1))\n",
    "        for i in np.arange(size):\n",
    "            one_hot[i][labels[i]] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def preprocess(paths, centered=False):\n",
    "        size = len(paths)\n",
    "        result = []\n",
    "        for i in range(size):\n",
    "            foto = misc.imread(paths[i])\n",
    "            scale = 256/(min(foto.shape[:2]))\n",
    "            foto = misc.imresize(foto, scale)\n",
    "            if centered:\n",
    "                x = int(np.floor((foto.shape[0]-224)/2))\n",
    "                y = int(np.floor((foto.shape[1]-224)/2))            \n",
    "            else:\n",
    "                x = np.random.randint(0,foto.shape[0]-224)\n",
    "                y = np.random.randint(0,foto.shape[1]-224)\n",
    "            result.append(foto[x:224+x,y:224+y,:])\n",
    "        return np.array(result)\n",
    "    \n",
    "    def next_batch(self):\n",
    "        assert self.train_next <= len(self.train[0])\n",
    "        if self.train_next == len(self.train[0]):\n",
    "            self.train_next = 0\n",
    "        if self.train_next + self.batch > len(self.train[0]):\n",
    "            batch =  self.train[0][self.train_next:], self.train[1][self.train_next:]\n",
    "        else:\n",
    "            batch = self.train[0][self.train_next:self.train_next + self.batch], self.train[1][self.train_next:self.train_next + self.batch]\n",
    "        self.train_next += len(batch[0])\n",
    "        return batch\n",
    "    \n",
    "    def vset(self):\n",
    "        return self.valid\n",
    "    \n",
    "    def tset(self):\n",
    "        return self.test\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.train_next = 0\n",
    "        perm = np.random.permutation(len(self.train[0]))\n",
    "        self.train = self.train[0][perm], self.train[1][perm]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.train = self.starting\n",
    "        np.random.seed(self.seed)\n",
    "        self.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import glob\n",
    "\n",
    "ids = loadmat('setid.mat')\n",
    "\n",
    "test = ids['trnid'][0] -1\n",
    "train = ids['tstid'][0] -1\n",
    "valid = ids['valid'][0] - 1\n",
    "\n",
    "labels = loadmat('imagelabels.mat')['labels'][0]\n",
    "labels -= 1\n",
    "files = sorted(glob.glob('jpg/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "        \n",
    "data = Dataset(files, labels, train, test, valid, 50)\n",
    "validation = list(chunks(data.vset()[0], 50)), list(chunks(data.vset()[1], 50))\n",
    "test = list(chunks(data.tset()[0], 50)), list(chunks(data.tset()[1], 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "lr = tf.placeholder(tf.float32)\n",
    "x = tf.placeholder(tf.float32, [None, 224*224*3])\n",
    "x_image = tf.reshape(x, [-1,224,224,3])\n",
    "\n",
    "def conv2d(x, W, b, strides=[1,1,1,1], padding=[[0,0],[0,0],[0,0],[0,0]]):\n",
    "    x_padded = tf.pad(x, padding, 'CONSTANT')\n",
    "    h_conv = tf.nn.conv2d(x_padded, W, strides, 'VALID') + b\n",
    "    normed = tf.contrib.layers.batch_norm(h_conv, epsilon=1e-3)\n",
    "    return tf.nn.relu(normed)\n",
    "\n",
    "def max_pool(x, ksize, strides):\n",
    "    return tf.nn.max_pool(x, ksize=ksize, strides=strides, padding='VALID')\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    W_conv1 = weight_variable([11, 11, 3, 96])\n",
    "    b_conv1 = bias_variable([96])\n",
    "    h_conv1 = conv2d(x_image, W_conv1, b_conv1, [1, 4, 4, 1], [[0,0],[2,2],[2,2],[0,0]])\n",
    "    h_pool1 = max_pool(h_conv1, [1,3,3,1], [1,2,2,1])\n",
    "\n",
    "    W_conv2 = weight_variable([5, 5, 96, 256])\n",
    "    b_conv2 = bias_variable([256])\n",
    "    h_conv2 = conv2d(h_pool1, W_conv2, b_conv2, [1, 1, 1, 1], [[0,0],[2,2],[2,2],[0,0]])\n",
    "    h_pool2 = max_pool(h_conv2, [1,3,3,1], [1,2,2,1])\n",
    "\n",
    "    W_conv3 = weight_variable([3, 3, 256, 384])\n",
    "    b_conv3 = bias_variable([384])\n",
    "    h_conv3 = conv2d(h_pool2, W_conv3, b_conv3, [1, 1, 1, 1], [[0,0],[1,1],[1,1],[0,0]])\n",
    "\n",
    "    W_conv4 = weight_variable([3, 3, 384, 384])\n",
    "    b_conv4 = bias_variable([384])\n",
    "    h_conv4 = conv2d(h_conv3, W_conv4, b_conv4, [1, 1, 1, 1], [[0,0],[1,1],[1,1],[0,0]])\n",
    "\n",
    "    W_conv5 = weight_variable([3, 3, 384, 256])\n",
    "    b_conv5 = bias_variable([256])\n",
    "    h_conv5 = conv2d(h_conv4, W_conv5, b_conv5, [1, 1, 1, 1], [[0,0],[1,1],[1,1],[0,0]])\n",
    "    h_pool3 = max_pool(h_conv5, [1,3,3,1], [1,2,2,1])\n",
    "\n",
    "    prob = tf.placeholder(tf.float32)\n",
    "    h_drop1 = tf.nn.dropout(h_pool3, prob)\n",
    "\n",
    "    W_conv6 = weight_variable([6, 6, 256, 4096])\n",
    "    b_conv6 = bias_variable([4096])\n",
    "    h_conv6 = conv2d(h_drop1, W_conv6, b_conv6, [1, 1, 1, 1])\n",
    "\n",
    "    h_drop2 = tf.nn.dropout(h_conv6, prob)\n",
    "\n",
    "    W_conv7 = weight_variable([1, 1, 4096, 4096])\n",
    "    b_conv7 = bias_variable([4096])\n",
    "    h_conv7 = conv2d(h_drop2, W_conv7, b_conv7)\n",
    "\n",
    "    nClasses = 102\n",
    "    W_conv8 = weight_variable([1, 1, 4096, nClasses])\n",
    "    b_conv8 = bias_variable([nClasses])\n",
    "    y = conv2d(h_conv7, W_conv8, b_conv8)[:,0,0,:]\n",
    "\n",
    "    y_ = tf.placeholder(tf.float32, [None, nClasses])\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as time\n",
    "\n",
    "def debug(text, start):\n",
    "    print(text + \" Time spent: \" + str(time.now()-start).split(\".\")[0])\n",
    "\n",
    "maxim = 0\n",
    "sess = tf.Session()\n",
    "sess.as_default()\n",
    "rate = np.float(0.001)\n",
    "accuracies = []\n",
    "sess.run(tf.global_variables_initializer())\n",
    "start = time.now()\n",
    "data.reset()\n",
    "for k in range(420):\n",
    "    for i in range(123):\n",
    "        batch = data.next_batch()\n",
    "        train_step.run(session=sess, feed_dict={x_image: batch[0], y_: batch[1], prob: 0.5, lr: rate})\n",
    "        print(len(batch[0]))\n",
    "    if k % 10 == 0:\n",
    "        val = []\n",
    "        for i in range(len(validation[0])):\n",
    "            val.append(accuracy.eval(session=sess, feed_dict={x_image: validation[0][i], y_: validation[1][i], prob: 1}))\n",
    "        val = np.mean(val)\n",
    "        accuracies.append(val)\n",
    "        if maxim <= val:\n",
    "            maxim = val\n",
    "            save_path = saver.save(sess, \"model\" + \".ckpt\")    \n",
    "        debug(\"Learning rate: \" + str(rate) + \" Epoch: \" + str(k+1) + \" Accuracy: \" + str(val), start)\n",
    "    if k % 160 == 0 and k != 0:\n",
    "        rate *= 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
