{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from tensorflow.python.ops import nn_ops, array_ops\n",
    "\n",
    "def conv2d(x, W, strides=(1,1,1,1), padding='VALID'):\n",
    "  return tf.nn.conv2d(x, W, strides, padding)\n",
    "\n",
    "#Wraper podstawiajacy \n",
    "def py_func(func, inp, Tout, stateful=True, name=None):\n",
    "    \n",
    "    # Nowa nazwa, żeby móc modyfikować nasz gradient\n",
    "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "    tf.RegisterGradient(rnd_name)(_BinGrad(inp[2]))\n",
    "    \n",
    "    #Podstawienie gradientu\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
    "\n",
    "#Wrapper na wrappera\n",
    "def custom_conv2d(x, W, strides=(1,1,1,1), padding='VALID', name='conv'):\n",
    "    if padding == \"SAME\":\n",
    "        shape_x = tf.shape(x)[1:3]\n",
    "        shape_W = tf.shape(W)[0:2]\n",
    "        padh = shape_W[0] - shape_x[0] % shape_W[0]\n",
    "        padw = shape_W[1] - shape_x[1] % shape_W[1]\n",
    "        npad = ((0, 0), (0, padh), (0, padw), (0,0))\n",
    "        x = tf.pad(x, [[0,0],[0, padh],[0, padw], [0,0]])\n",
    "    return py_func(conv, [x,W,strides], [tf.float32])[0]\n",
    "    \n",
    "#Skopiowany gradient z conv2d\n",
    "def _BinGrad(strides):\n",
    "    return lambda op, grad: [nn_ops.conv2d_backprop_input(array_ops.shape(op.inputs[0]), \n",
    "                                                          op.inputs[1], \n",
    "                                                          grad, \n",
    "                                                          strides,\n",
    "                                                          'VALID', \n",
    "                                                          False, \n",
    "                                                          'NHWC'),\n",
    "                            nn_ops.conv2d_backprop_filter(op.inputs[0],\n",
    "                                                          array_ops.shape(op.inputs[1]), \n",
    "                                                          grad,\n",
    "                                                          strides,\n",
    "                                                          'VALID',\n",
    "                                                          False,\n",
    "                                                          'NHWC'),\n",
    "                            op.inputs[2]]\n",
    "\n",
    "#Nasza operacja\n",
    "#result[batch,0:conv_height,0:conv_width,w_filter] -> suma konwolucji wszystkich channeli dla konkretnego batcha\n",
    "#i filtra\n",
    "def conv(x, W, strides):\n",
    "    batches,x_height,x_width,x_channels = x.shape\n",
    "    W_height,W_width,w_channels,w_filters = W.shape\n",
    "    assert w_channels == x_channels\n",
    "    result = None\n",
    "    for batch in range(batches):\n",
    "        for w_filter in range(w_filters):\n",
    "            for channel in range(w_channels):\n",
    "                inp = x[batch,0:x_height,0:x_width,channel]\n",
    "                fil = np.rot90(W[0:W_height,0:W_width,channel,w_filter], 2)\n",
    "                conv_result = signal.convolve2d(inp, fil, 'valid')\n",
    "                conv_height,conv_width = conv_result.shape\n",
    "                if result is None:\n",
    "                    result = np.zeros((batches,conv_height,conv_width,w_filters), dtype=np.float32)\n",
    "                tmp = result[batch,0:conv_height,0:conv_width,w_filter]\n",
    "                result[batch,0:conv_height,0:conv_width,w_filter] = np.add(tmp, conv_result)\n",
    "    if not (strides==[1,1,1,1]).all():\n",
    "        b,h,w,c = result.shape\n",
    "        sh = strides[1]\n",
    "        sw = strides[2]\n",
    "        dh = [i for i in range(h) if i%sh != 0]\n",
    "        dw = [i for i in range(w) if i%sw != 0]\n",
    "        result = np.delete(result, dh, 1)\n",
    "        result = np.delete(result, dw, 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test na feed forward, jeden input, jeden channel, jeden weight filter\n",
    "def feedforward_1i_1ch_1w():\n",
    "    x = tf.placeholder(tf.float32, [None, 9])\n",
    "    x_reshaped = tf.reshape(x, [-1,3,3,1])\n",
    "    w = np.zeros([2,2,1,1])\n",
    "    w[0:2,0:2,0,0] = np.vstack(([1,0],[0,0]))\n",
    "    W_conv1 = tf.Variable(tf.cast(w,dtype=tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.as_default()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    numbers = np.reshape(np.array([1,2,3,4,5,6,7,8,9], dtype=np.float32),(1,9))\n",
    "    result = custom_conv2d(x_reshaped, W_conv1, strides=(1,2,2,1), padding='SAME')\n",
    "    result_n = conv2d(x_reshaped, W_conv1, strides=(1,2,2,1), padding='SAME')\n",
    "    result_conv = result.eval(session=sess, feed_dict={x: numbers})\n",
    "    result_norm = result_n.eval(session=sess, feed_dict={x: numbers})\n",
    "    assert result_conv.shape == result_norm.shape\n",
    "    assert np.linalg.norm(result_conv-result_norm) < 0.01\n",
    "feedforward_1i_1ch_1w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3][1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "#test na feed forward, jeden input, jeden channel, jeden weight filter\n",
    "def feedforward_1i_1ch_1w():\n",
    "    x = tf.placeholder(tf.float32, [None, 42])\n",
    "    x_reshaped = tf.reshape(x, [-1,7,6,1])\n",
    "    w = np.zeros([3,4,1,1])\n",
    "    #w[0:2,0:2,0,0] = np.vstack(([1,0],[0,0]))\n",
    "    W_conv1 = tf.Variable(tf.cast(w,dtype=tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.as_default()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    numbers = np.reshape(np.zeros(42, dtype=np.float32),(1,42))\n",
    "    result_n = conv2d(x_reshaped, W_conv1, strides=[1,2,4,1], padding='VALID')\n",
    "    result_norm = result_n.eval(session=sess, feed_dict={x: numbers})\n",
    "    print(result_norm.shape)\n",
    "feedforward_1i_1ch_1w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test na feed forward, jeden input, dwa channele, dwa weight filter\n",
    "def feedforward_1i_2ch_2w():\n",
    "    x = tf.placeholder(tf.float32, [None, 9])\n",
    "    x_reshaped = tf.reshape(x, [-1,3,3,2])\n",
    "    w = np.zeros([2,2,2,2])\n",
    "    w[0:2,0:2,0,0] = np.vstack(([1,1],[1,1]))\n",
    "    w[0:2,0:2,1,0] = np.vstack(([1,1],[1,1]))\n",
    "    w[0:2,0:2,0,1] = np.vstack(([1,1],[1,1]))\n",
    "    W_conv1 = tf.Variable(tf.cast(w,dtype=tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.as_default()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    numbers = np.reshape(np.array([1,2,3,4,5,6,7,8,9], dtype=np.float32),(1,9))\n",
    "    inputs = np.vstack((numbers,numbers));\n",
    "    result = custom_conv2d(x_reshaped, W_conv1)\n",
    "    result_n = conv2d(x_reshaped, W_conv1)\n",
    "    result_conv = result.eval(session=sess, feed_dict={x: inputs})\n",
    "    result_norm = result_n.eval(session=sess, feed_dict={x: inputs})\n",
    "    assert result_conv.shape == result_norm.shape\n",
    "    assert np.linalg.norm(result_conv-result_norm) < 0.01\n",
    "feedforward_1i_2ch_2w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test na feed forward, jeden input, dwa channele, dwa weight filter\n",
    "def feedforward_2i_2ch_2w():\n",
    "    x = tf.placeholder(tf.float32, [None, 9])\n",
    "    x_reshaped = tf.reshape(x, [-1,3,3,2])\n",
    "    w = np.zeros([2,2,2,2])\n",
    "    w[0:2,0:2,0,0] = np.vstack(([1,1],[1,1]))\n",
    "    w[0:2,0:2,1,0] = np.vstack(([1,1],[1,1]))\n",
    "    w[0:2,0:2,0,1] = np.vstack(([1,1],[1,1]))\n",
    "    W_conv1 = tf.Variable(tf.cast(w,dtype=tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.as_default()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    numbers = np.reshape(np.array([1,2,3,4,5,6,7,8,9], dtype=np.float32),(1,9))\n",
    "    inputs = np.vstack((numbers,numbers,numbers,numbers));\n",
    "    result = custom_conv2d(x_reshaped, W_conv1)\n",
    "    result_n = conv2d(x_reshaped, W_conv1)\n",
    "    result_conv = result.eval(session=sess, feed_dict={x: inputs})\n",
    "    result_norm = result_n.eval(session=sess, feed_dict={x: inputs})\n",
    "    assert result_conv.shape == result_norm.shape\n",
    "    assert np.linalg.norm(result_conv-result_norm) < 0.01\n",
    "feedforward_2i_2ch_2w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test na gradienty, jeden input, jeden channel, jeden weight filter\n",
    "def gradient_1i_1ch_1w():\n",
    "    x = tf.placeholder(tf.float32, [None, 4])\n",
    "    x_reshaped = tf.reshape(x, [-1,2,2,1])\n",
    "    w = np.zeros([2,2,1,1])\n",
    "    w[0:2,0:2,0,0] = np.vstack(([0.1,0.2],[1,0.3]))\n",
    "    W_conv1 = tf.Variable(tf.cast(w,dtype=tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.as_default()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    numbers = np.reshape(np.array([2,1,1,1], dtype=np.float32),(1,4))\n",
    "    result = custom_conv2d(x_reshaped, W_conv1)\n",
    "    result_n = conv2d(x_reshaped, W_conv1)\n",
    "\n",
    "    var_grad = tf.gradients(result, W_conv1)[0]\n",
    "    var_grad_n = tf.gradients(result_n, W_conv1)[0]\n",
    "    gradients = var_grad.eval(session=sess, feed_dict={x: numbers})\n",
    "    gradients_n = var_grad_n.eval(session=sess, feed_dict={x: numbers})\n",
    "    assert np.linalg.norm(gradients-gradients_n) < 0.01\n",
    "gradient_1i_1ch_1w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test na feed forward, jeden input, dwa channele, dwa weight filter\n",
    "def gradient_1i_2ch_2w():\n",
    "    x = tf.placeholder(tf.float32, [None, 4])\n",
    "    x_reshaped = tf.reshape(x, [-1,2,2,2])\n",
    "    w = np.zeros([2,2,2,2])\n",
    "    w[0:2,0:2,0,0] = np.vstack(([1,1],[1,1]))\n",
    "    w[0:2,0:2,1,0] = np.vstack(([1,1],[1,1]))\n",
    "    w[0:2,0:2,0,1] = np.vstack(([1,1],[1,1]))\n",
    "    W_conv1 = tf.Variable(tf.cast(w,dtype=tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.as_default()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    numbers = np.reshape(np.array([1,2,3,4], dtype=np.float32),(1,4))\n",
    "    inputs = np.vstack((numbers,numbers, numbers, numbers));\n",
    "    result = custom_conv2d(x_reshaped, W_conv1)\n",
    "    result_n = conv2d(x_reshaped, W_conv1)\n",
    "    \n",
    "    var_grad = tf.gradients(result, W_conv1)[0]\n",
    "    var_grad_n = tf.gradients(result_n, W_conv1)[0]\n",
    "    gradients = var_grad.eval(session=sess, feed_dict={x: inputs})\n",
    "    gradients_n = var_grad_n.eval(session=sess, feed_dict={x: inputs})\n",
    "    assert np.linalg.norm(gradients-gradients_n) < 0.01\n",
    "gradient_1i_2ch_2w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2a02e2057ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mgradients_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_grad_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgradients_n\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgradient_2i_2ch_2w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-2a02e2057ba2>\u001b[0m in \u001b[0;36mgradient_2i_2ch_2w\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mresult_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_conv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'VALID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mvar_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_conv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mvar_grad_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_conv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[0;31m# If grad_fn was found, do not use SymbolicGradient even for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                 \u001b[0min_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-4b8a7e61b045>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m     47\u001b[0m                                                           \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                           'NHWC'),\n\u001b[0;32m---> 49\u001b[0;31m                             op.inputs[2], op.inputs[3]]\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#Nasza operacja\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#test na feed forward, dwa inputy, dwa channele, dwa weight filter\n",
    "def gradient_2i_2ch_2w():\n",
    "    x = tf.placeholder(tf.float32, [None, 4])\n",
    "    x_reshaped = tf.reshape(x, [-1,2,2,2])\n",
    "    w = np.zeros([2,2,2,2])\n",
    "    w[0:2,0:2,0,0] = np.vstack(([1,1],[1,1]))\n",
    "    w[0:2,0:2,1,0] = np.vstack(([1,1],[1,1]))\n",
    "    w[0:2,0:2,0,1] = np.vstack(([1,1],[1,1]))\n",
    "    W_conv1 = tf.Variable(tf.cast(w,dtype=tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.as_default()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    numbers = np.reshape(np.ones(4, dtype=np.float32),(1,4))\n",
    "    inputs = np.vstack((numbers,numbers,numbers,numbers));\n",
    "    result = custom_conv2d(x_reshaped, W_conv1, strides=[1,2,2,1], padding='VALID')\n",
    "    result_n = conv2d(x_reshaped, W_conv1, strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    var_grad = tf.gradients(result, W_conv1)[0]\n",
    "    var_grad_n = tf.gradients(result_n, W_conv1)[0]\n",
    "    gradients = var_grad.eval(session=sess, feed_dict={x: inputs})\n",
    "    gradients_n = var_grad_n.eval(session=sess, feed_dict={x: inputs})\n",
    "    assert np.linalg.norm(gradients-gradients_n) < 0.01\n",
    "gradient_2i_2ch_2w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.],\n",
       "         [ 1.],\n",
       "         [ 1.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.]],\n",
       "\n",
       "        [[ 1.],\n",
       "         [ 1.],\n",
       "         [ 1.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.]],\n",
       "\n",
       "        [[ 1.],\n",
       "         [ 1.],\n",
       "         [ 1.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.]],\n",
       "\n",
       "        [[ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.]],\n",
       "\n",
       "        [[ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32, [9])\n",
    "x = tf.reshape(a, [1,3,3,1])\n",
    "padded = tf.pad(x, [[0,0],[0,2],[0,3],[0,0]])\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.as_default()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num = np.ones(9)\n",
    "tf.shape(padded).eval(session=sess, feed_dict = {a: num})\n",
    "padded.eval(session=sess, feed_dict = {a: num})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
